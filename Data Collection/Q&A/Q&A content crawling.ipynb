{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import jieba\n",
    "import time\n",
    "import csv\n",
    "import random\n",
    "import sys\n",
    "import codecs\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "        reload(sys)\n",
    "        sys.setdefaultencoding('utf-8')\n",
    "\n",
    "headers = {\n",
    "        'Authority':'www.mafengwo.cn',\n",
    "        'Accept': '*/*',\n",
    "        'Accept-Encoding': 'gzip, deflate,br',\n",
    "        'Accept-Language': 'zh-CN,zh;q=0.9',\n",
    "        'Connection': 'keep-alive',\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/70.0.3538.77 Safari/537.36'\n",
    "        }\n",
    "#爬取组长数据\n",
    "def Crawl_leader_of_the_information():\n",
    "    firstPageUrl = 'http://www.mafengwo.cn/g/11337.html'    #第一页网页地址,小组成员页\n",
    "    res = requests.get(firstPageUrl, headers=headers)       #get请求网页信息\n",
    "    soup_bs = BeautifulSoup(res.text, 'lxml')               #BeautifulSoup获取html信息\n",
    "    dic_leader = {}                #dic_leader用于保存组长信息,不知道组员里是否有组长信息,所以单独爬取\n",
    "    try:\n",
    "        leader = soup_bs.select('ul.bd > li:nth-of-type(1) > p:nth-of-type(1) > a')  #新方法,理解成:ul标签,类为bd内的第一个li标签内的第一个p标签内的a组\n",
    "        dic_leader['name'] = leader[0].text       #获取第一个a组内容,及组长名称\n",
    "        dic_leader['href'] = 'http://www.mafengwo.cn/wenda' + leader[0].get('href')  #获取a组的href属性\n",
    "    \n",
    "    except:\n",
    "        print('Error')    #请求失败\n",
    "    return dic_leader\n",
    "def Crawl_members_of_the_information(i):\n",
    "    ary_members = []       #储存所有组员的信息\n",
    "    pageUrl = 'http://www.mafengwo.cn/g/11337.html?action=get_members&page={}&gid=11337'.format(i)   #把i页放入地址,生成json请求地址,可在浏览器的F12内查看\n",
    "    res = requests.get(pageUrl, headers=headers)     #请求json数据\n",
    "    try:\n",
    "        json_html = json.loads(res.text)   #把json数据转换成html,json_html为字典\n",
    "        html = json_html['html']           #获取json_html字典key:html的数据\n",
    "        soup_bs = BeautifulSoup(html,'lxml')\n",
    "        members = soup_bs.find('ul',class_='bd list-member')\n",
    "        for member in members.find_all('li'):     #爬取组员数据\n",
    "            dic_member = {}\n",
    "            member_data = member.select('p > a')\n",
    "            dic_member['name'] = member_data[0].text   #获取组员名称\n",
    "            dic_member['href'] = 'http://www.mafengwo.cn/wenda' + member_data[0].get('href')   #获取href后,合并网站头http://www.mafengwo.cn,并加上/wenda,后面请求问答的页面用\n",
    "            ary_members.append(dic_member)\n",
    "    except:\n",
    "        print('Error')   #请求失败\n",
    "    return ary_members\n",
    "#请求每个组员提问和回答的数量\n",
    "\n",
    "\n",
    "\n",
    "def Crawl_question_and_answer(dic_member):\n",
    "    dic_members =[]\n",
    "    dic_qmembers=[]\n",
    "    dic_amembers=[]\n",
    "    question_url = dic_member['href'][:-5] + '/question.html'  #组合完整的组员提问网页地址\n",
    "    answer_url = dic_member['href'][:-5] + '/answer.html'      #组合完整的组员回答网页地址\n",
    "    #question\n",
    "    res = requests.get(question_url, headers=headers)\n",
    "    soup_bs = BeautifulSoup(res.text, 'lxml')\n",
    "    #str_name=soup_bs.find('div',class_='MAvaName').strip('\\n').text\n",
    "    #dic_member['nickname']=str_name\n",
    "    str_question = soup_bs.find('span',class_='_j_total_item').text\n",
    "    dic_member['question'] = int(str_question[:-1])        #删除'个'字,并字符转换为整数\n",
    "    time.sleep(random.randint(2,5))#通过生成随机数,来延时请求,防止反爬,短时间内大量请求会被封IP,导致无法爬取\n",
    "    str_qsession=soup_bs.find_all('div', attrs={'class':\"wd-item\"})\n",
    "    i=1\n",
    "    for qsession in str_qsession:\n",
    "        dic_qmember={}\n",
    "        dic_qmember['Qno.']=i\n",
    "        dic_member['questionhref']='http://www.mafengwo.cn'+(qsession.find('div',class_='wd-title')).find('a')['href']\n",
    "        dic_qmember['questiontitle']=qsession.find('a').text\n",
    "        #dic_qmember['questioncontext']=qsession.find('div',class_='wd-detail').text\n",
    "        i+=1\n",
    "        dic_qmembers.append(dic_qmember)\n",
    "    dic_member['questionhref']=str(dic_qmembers)\n",
    "    #print(dic_member['Qcontext'])\n",
    "    #answer\n",
    "    res = requests.get(answer_url, headers=headers)\n",
    "    soup_bs = BeautifulSoup(res.text, 'lxml')\n",
    "    str_answer = soup_bs.find('span',class_='_j_total_item').text\n",
    "    dic_member['answer'] = int(str_answer[:-1])\n",
    "    time.sleep(random.randint(2,5))\n",
    "    str_asession=soup_bs.find_all('div', attrs={'class':\"wd-item\"})\n",
    "    q=1\n",
    "    for asession in str_asession:\n",
    "        dic_amember={}\n",
    "        #dic_amember['answer'] = int(str_answer[:-1])\n",
    "        dic_amember['Ano,']=q\n",
    "        dic_amember['answerhref']='http://www.mafengwo.cn'+(asession.find('div',class_='wd-title')).find('a')['href']\n",
    "        dic_amember['answertitle']=asession.find('a').text\n",
    "        #dic_amember['answercontext']=asession.find('div',class_='wd-detail').text\n",
    "        q+=1\n",
    "        dic_amembers.append(dic_amember)\n",
    "    dic_member['answerhref']=str(dic_amembers)\n",
    "    dic_members.append(dic_member)\n",
    "    print(dic_member)\n",
    "\n",
    "\n",
    "    \n",
    "#打印无提问和回答数的组员\n",
    "club_data = []      #全部人员的信息\n",
    "club_data.append(Crawl_leader_of_the_information())         #爬组长的信息(名字,连接)\n",
    "for i in range(1,17):\n",
    "    club_data.extend(Crawl_members_of_the_information(i))#爬组员的信息(名字,连接)\n",
    "\n",
    "for i in range(0,100):\n",
    "    Crawl_question_and_answer(club_data[i])                #通过连接,爬取提问和回答数，标题和内容\n",
    "\n",
    "\n",
    "with open(\"test.csv\", 'w', newline='', encoding='utf-8-sig') as csvfile:\n",
    "    filednames = ['name','question', 'questionhref', 'answer', 'answerhref','href']\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=filednames)\n",
    "    writer.writeheader()\n",
    "    writer.writerow(club_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
